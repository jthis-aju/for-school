import os
import numpy as np
import cv2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.applications.efficientnet import EfficientNetB0
from keras.models import Sequential
from keras.layers import Dense, Dropout, GlobalAveragePooling2D
from keras.optimizers import Adam
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import seaborn as sns
from sklearn.preprocessing import label_binarize

# Define paths and image size
base_path = "/content/drive/MyDrive/archive/"  # Path to the folder containing image subfolders
img_size = (224, 224)  # Input size for EfficientNetB0

# Function to load images and labels
def load_images_and_labels(base_path, img_size=(224, 224)):
    images = []
    labels = []
    for label_name in os.listdir(base_path):
        label_path = os.path.join(base_path, label_name)
        if os.path.isdir(label_path):
            for img_file in os.listdir(label_path):
                img_path = os.path.join(label_path, img_file)
                # Load and resize image
                img = cv2.imread(img_path)
                if img is not None:
                    img = cv2.resize(img, img_size)
                    images.append(img)
                    labels.append(label_name)
    return np.array(images), np.array(labels)

# Load images and labels
X_combined, y_combined_labels = load_images_and_labels(base_path, img_size)

# Encode labels
label_encoder = LabelEncoder()
y_combined_encoded = label_encoder.fit_transform(y_combined_labels)
y_combined_one_hot = to_categorical(y_combined_encoded)

# Save the encoded labels and images (optional)
np.save("normalized_combined_images.npy", X_combined)
np.save("labels_combined.npy", y_combined_encoded)

# Split the dataset into training, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X_combined, y_combined_one_hot, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Define the data augmentation generator for training
train_datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True
)

# Apply data augmentation to the training set
train_generator = train_datagen.flow(X_train, y_train, batch_size=32)

# Load the EfficientNetB0 model with pre-trained weights
base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=X_train.shape[1:])

# Freeze the base model layers
base_model.trainable = False

# Add custom classification layers
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(len(label_encoder.classes_), activation='softmax')
])

# Compile the model
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model
history = model.fit(
    train_generator,
    validation_data=(X_val, y_val),
    epochs=10,
    batch_size=32
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Save the trained model
model.save('efficientnet_mri_classification.h5')

# Plot training and validation accuracy and loss
def plot_training_history(history):
    # Accuracy plot
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss plot
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Call the function
plot_training_history(history)

# Generate predictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

# Compute confusion matrix
conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)

# Plot confusion matrix
def plot_confusion_matrix(conf_matrix, class_names):
    plt.figure(figsize=(10, 8))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

# Call the function
plot_confusion_matrix(conf_matrix, label_encoder.classes_)

# Plot sample predictions
def plot_sample_predictions(X_test, y_true_classes, y_pred_classes, label_encoder, num_samples=10):
    plt.figure(figsize=(15, 10))
    for i in range(num_samples):
        idx = np.random.randint(0, len(X_test))
        plt.subplot(2, 5, i + 1)
        plt.imshow(cv2.cvtColor(X_test[idx], cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for display
        plt.title(f"True: {label_encoder.classes_[y_true_classes[idx]]}\nPred: {label_encoder.classes_[y_pred_classes[idx]]}")
        plt.axis('off')
    plt.tight_layout()
    plt.show()

# Call the function
plot_sample_predictions(X_test, y_true_classes, y_pred_classes, label_encoder)

# Plot ROC curve
# Binarize the labels for ROC curve
y_test_binarized = label_binarize(y_true_classes, classes=range(len(label_encoder.classes_)))

plt.figure(figsize=(10, 8))
for i, class_name in enumerate(label_encoder.classes_):
    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{class_name} (AUC = {roc_auc:.2f})')

plt.title('ROC Curve for Each Class')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.grid()
plt.show()
